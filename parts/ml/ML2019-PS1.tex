
	\maketitle
	\numberwithin{equation}{section}	
	\section{[20pts] Basic Probability and Statistics}
	The probability distribution of random variable $X$ follows:\\
	\begin{equation}
	f_X(x)=\begin{cases}
	\frac{1}{2} & 0<x<1;\\
	\frac{1}{6} & 2<x<5;\\
	0 & \text{otherwise}.
	\end{cases}
	\end{equation} 
	(1) [5pts] Please give the cumulative distribution function $F_X(x)$ for X;\\ 

	\begin{solution}
		\begin{equation}
			F_X(x)=\begin{cases}
				0 & x \leq 0 \\
				\frac{1}{2}x & 0 < x < 1 \\
				\frac{1}{2} & 1 \leq x \leq 2 \\
				\frac{1}{6}x+\frac{1}{6} & 2 < x < 5 \\
				1 & 5 < x
			\end{cases}\notag
		\end{equation}
	\end{solution}
	
	(2) [5pts] Define random variable $Y$ as $Y=1/(X^2)$, please give the probability density function $f_Y(y)$ for $Y$;\\ 
	
	\begin{solution}
		
		\begin{equation}
			F_y(y)=1+F_x(-y^{-\frac{1}{2}})-F_x(y^{-\frac{1}{2}}))\notag
		\end{equation}

		So we have $f_y(y)=\frac{1}{2}y^{-\frac{3}{2}}f_x(y^{-\frac{1}{2}})$. Thus 
		\begin{equation}
			f_Y(y)=\begin{cases}
				\frac{1}{12}y^{-\frac{3}{2}} & \frac{1}{25} < y < \frac{1}{4} \\
				\frac{1}{4}y^{-\frac{3}{2}} & 1 < y \\
				0 & \text{otherwise}
			\end{cases}
		\end{equation}
	\end{solution}
			

	(3) [10pts] For some random non-negative random variable Z, please prove the following two formulations are equivalent:\\
	\begin{equation}
	\mathbb{E}[Z]=\int^\infty_{z=0} z f(z)\mathrm{d}z,
	\end{equation}
	\begin{equation}
	\mathbb{E}[Z]=\int^\infty_{z=0} \mathrm{Pr}[Z\geq z]\mathrm{d}z,
	\end{equation}
	Meantime, please calculate the expectation of random variable $X$ and $Y$ by these two expectation formulations to verify your proof.

	\begin{solution}
		Notice the fact that
		\[\int_0^Z dt=\int_0^\infty [t<Z]dt\]
		where $[t<Z]=1$ if $t<Z$ while $[t<Z]=0$ if $t>Z$.

		Thus we have
		\begin{align}
			\mathbb{E}[Z] & =\int_0^\infty zf(z)dz = \int_0^\infty (\int_0^z dt)f(z)dz = \int_0^\infty(\int_0^\infty [t<z]dt)f(z) dz\notag\\
			& = \int_0^\infty\int_0^\infty f(z)[z>t] dzdt =\int_0^\infty\int_t^\infty f(z) dzdt = \int_0^\infty (1-F(t))dt\notag\\
			& = \int_0^\infty Pr(Z\geq z)dz\notag
		\end{align}

		For $X$, we have

		\begin{align}
			\mathbb{E}[X] & =\int^\infty_{x=0} x f(x)\mathrm{d}x = \int_0^1 \frac{1}{2}x dx + \int_2^5 \frac{1}{6}x dx = 2\notag\\
			\mathbb{E}[X] & =\int^\infty_{x=0} \mathrm{Pr}[X\geq x]\mathrm{d}x = \int_0^1(1-\frac{1}{2}x)+\int_1^2\frac{1}{2}+\int_2^5(\frac{5}{6}-\frac{1}{6}x) = 2\notag
		\end{align}

		For $Y$, we have

		\begin{align}
			\mathbb{E}[Y] & =\int^\infty_{y=0} y f(y)\mathrm{d}y = \int_{\frac{1}{25}}^{\frac{1}{4}} \frac{1}{12}y^{-\frac{1}{2}} + \int_\frac{1}{4}^\infty \frac{1}{4}y^{-\frac{1}{2}} = \infty\notag\\
			\mathbb{E}[Y] & =\int^\infty_{y=0} \mathrm{Pr}[Y\geq y]\mathrm{d}y = \infty\notag
		\end{align}

	\end{solution}

	\section{[20pts] Strong Convexity}
	Let $D\in \mathbb{R}^2$ be a finite set. Define a function $E: \mathbb{R}^3 \rightarrow \mathbb{R}$ by\\
	\begin{equation}
	E(a,b,c)=\sum\limits_{x\in\mathcal{D}}(ax^2_1+bx_1+c-x_2)^2.
	\end{equation}
	(1) [10pts] Show that $E$ is convex.\\
	\begin{solution}
		Let $A=a_1x_1^2+b_1x_1+c_1-x_2, B=a_2x_1^2+b_2x_1+c_2-x_2,t\in [0,1]$

		\begin{align}
			& E(ta_1+(1-t)a_2,tb_1+(1-t)b_2,tc_1+(1-t)c_2)\notag \\
			&=\sum ((ta_1+(1-t)a_2)x_1^2+(tb_1+(1-t)b_2)x_1+(tc_1+(1-t)c_2)-tx_2-(1-t)x_2)^2\notag\\
			&= \sum (tA+(1-t)B)^2 \notag
		\end{align}
		Thus we have
		\begin{align}
			& tE(a_1,b_1,c_1) + (1-t)E(a_2,b_2,c_2) - E(ta_1+(1-t)a_2,tb_1+(1-t)b_2,tc_1+(1-t)c_2)\notag\\
			& = \sum (tA^2+(1-t)B^2-t^2A^2-(1-t)^2B^2-2AB(1-t)t) \notag\\
			& = \sum (t(1-t)(A-B)^2) \geq 0\notag
		\end{align}

		Thus by definition of convex function, we have proved that $E$ is convex.
	\end{solution}

	(2) [10pts] Does there exist a set $D$ such that $E$ is strongly convex? Proof or a counterexample.

	\begin{solution}
		The Hessian matrix of $E$ is
		\begin{equation}
			H(E) = 2*\begin{bmatrix}
				\sum x_1^4 & \sum x_1^3 & \sum x_1^2 \\
				\sum x_1^3 & \sum x_1^2 & \sum x_1\\
				\sum x_1^2 & \sum x_1 & \sum 1
			\end{bmatrix}\notag
		\end{equation}

		Let $D=\{(1,0),(2,0),(3,0),(4,0),(-1,0),(-2,0),(-3,0),(-4,0)\}$. Then 
		\begin{equation}
			H(E) = 2*\begin{bmatrix}
				708 & 0 & 60 \\
				0 & 60 & 0\\
				60 & 0 & 8
			\end{bmatrix}\notag
		\end{equation}

		The sequential principal minor equals to $8*708,8*708*60,8*123840$ respectively. Thus $H(E)$ is positive definite. Thus there exist a set $D$ that $E$ is strongly convex.

	\end{solution}
	
	\section{[20pts] Transition Probability Matrix}
	Suppose $x_k$ is the fraction of NJU students who prefer course A at year $k$. The remaining fraction $y_k = 1-x_k$ prefers course B. \\\\
	At year $k + 1$, $\frac{1}{5}$ of those who prefer course A change their mind. Also at the same year, $\frac{1}{10}$ of those who prefer course B change their mind (possibly after taking the problem 3 last year). \\ \\ 
	Create the matrix P to give $[x_{k+1}\quad y_{k+1}]^\top = P [x_k\quad y_k]^\top$ and find the limit of $P^k[1\quad 0]^\top$ as $k \rightarrow \infty$.

	\begin{solution}
		\begin{equation}
			P=\begin{bmatrix}
				\frac{4}{5} & \frac{1}{10} \\
				\frac{1}{5} & \frac{9}{10} 
			\end{bmatrix} \notag
		\end{equation}
		
		The eigenvalue and corresponding eigen vector is
		\begin{align}
			\lambda_1& =1 & v_1 &=[1\ 2]^T\notag\\
			\lambda_2& =2 & v_2 &=[1\ -1]^T\notag
		\end{align}

		\begin{equation}
			\lim_{k\rightarrow\infty} P^k[1\ 0]^T = \lim_{k\rightarrow\infty} P^k(\frac{1}{3}v_1+\frac{2}{3}v_2) = \lim_{k\rightarrow\infty} (\frac{1}{3}v_1+(\frac{7}{10})^k*\frac{2}{3}v_2) = [\frac{1}{3}\ \frac{2}{3}]^T \notag
		\end{equation}
	\end{solution}
	
	\section{[20pts] Hypothesis Testing}
	Yesterday, a student was caught by the teacher when tossing a coin in class. The teacher is very nice and did not want to make things difficult. S(he) wished the student to determine \emph{if the coin is biased for heads} with $\alpha = 0.05$.\\ \\
	Also, according to the studentâ€™s desk mate, the coin was tossed for $50$ times and it got $35$ heads. \\ \\
	(1) [10pts] Show all calculate and rules (hint: using z-test). \\
	\begin{solution}
		Suppose the probability of tossing a coin for head is $p$.

		The null hypothesis $H_0$ is: $p<=\frac{1}{2}$. And the alternate hypothesis $H_1$ is: $p>\frac{1}{2}$. Since this event is a Bernoulli distribution, we have $\sigma^2=\sqrt{p(1-p)}=\frac{1}{2}$

		The rejection field is $z=\frac{\overline{p}-\frac{1}{2}}{\sigma/\sqrt{n}}>z_{\alpha}=1.645$.
		
		Now we have $z=\frac{0.7-0.5}{0.5/\sqrt{50}}=2.828>z_{\alpha}$. Thus we reject $H_0$ and shows that the coin is biased for heads under $\alpha=0.05$.
	\end{solution}
	
	(2) [10pts] Calculate the p-value and interpret it.

	\begin{solution}
		p-value$=P(Z\geq 2.828)=1-\Phi(2.828)=0.0024$. If $\alpha\geq 0.0024$, then we reject $H_0$. If $\alpha\leq 0.0024$, then we reject $H_1$. p-value is the area under the normal distribution on the right side of $2.828$.
	\end{solution}
	
	\section{[20pts] Performance Measures}
	We have a set of samples that we wish to classify in one of two classes and a ground truth class of each sample (denoted as 0 and 1). For each example a classifier gives us a score (score closer to 0 means class 0, score closer to 1 means class 1). Below are the results of two classifiers ($C_1$ and $C_2$) for 8 samples,their ground truth values ($y$) and the score values for both classifiers ($y_{C_1}$ and $y_{C_2}$).
	\begin{table}[htbp]
		\centering
		\begin{tabular}{c|cccccccc}
			\hline
			$y$ & 1 & 0 & 1 & 1 & 1 & 0 & 0 & 0\\
			\hline
			$y_{C_1}$ & 0.5 & 0.3 & 0.6 & 0.22 & 0.4 & 0.51 & 0.2 & 0.33\\
			\hline
			$y_{C_2}$ & 0.04 & 0.1 & 0.68 & 0.22 & 0.4 & 0.11 & 0.8 & 0.53\\
			\hline
		\end{tabular}
	\end{table}
	
	\noindent{(1) [8pts] For the example above calculate and draw the ROC curves for classifier $C_1$ and $C_2$. Also calculate the area under the curve (AUC) for both classifiers.}\\

	
	
	(2) [8pts] For the classifier $C_1$ select a decision threshold $th_1 = 0.33$ which means that $C_1$ classifies a sample as class 1, if its score $y_{C_1} > th_1$, otherwise it classifies it as class 0. Use it to calculate the confusion matrix and the $F_1$ score. Do the same thing for the classifier $C_2$ using a threshold value $th_2 = 0.1$.\\

	\begin{solution}
		For $C_1$, $th_1=0.33$, the F1 score is $\frac{3}{4}$. The confusion matrix is:
		\begin{table}[htbp]
			\centering
			\begin{tabular}{|c|c|c|}
				\hline
				\multirow{2}{*}{Ground Truth} & \multicolumn{2}{c|}{Prediction} \\ \cline{2-3}
				 & positive & negative \\ \hline
				True & 3 & 1 \\ \hline
				False & 1 & 3 \\ \hline
			\end{tabular}
			\caption{$C_1$}
		\end{table}

		For $C_2$, $th_2=0.1$, the F1 score is $\frac{3}{5}$. The confusion matrix is:
		\begin{table}[htbp]
			\centering
			\begin{tabular}{|c|c|c|}
				\hline
				\multirow{2}{*}{Ground Truth} & \multicolumn{2}{c|}{Prediction} \\ \cline{2-3}
				 & positive & negative \\ \hline
				True & 3 & 1 \\ \hline
				False & 3 & 1 \\ \hline
			\end{tabular}
			\caption{$C_2$}
		\end{table}
	\end{solution}

	(3) [4pts] Prove Eq.(2.22) in Page 35. (AUC = $1 - \ell_{rank}$).

	\begin{solution}
		Relabel the sample as $z$ in order not to make confusion. Suppose $z_1,\cdots,z_n$ is sorted in ascending order under $f$. $[A]=1$ if $A$ is true otherwise $[A]=0$. $n(z)$ denotes the elements $z_{i+1}$ if $z=z_i$.

		By definition, we have
		\[x_i=\frac{\sum_{z^-\in D^-}[f(z^-)>f(z_i)]}{m^-}\]
		\[y_i=\frac{\sum_{z^+\in D^+}[f(z^+)>f(z_i)]}{m^+}\]

		Thus we have
		\begin{align}
			AUC & = \frac{1}{2}\frac{1}{m^-m^+}\sum_{i=1}^{m-1}(\sum_{z^-\in D^-}([f(z^-)>f(z_{i+1})]-[f(z^-)>f(z_i)])
			\notag \\ 
			&*\sum_{z^+\in D^+}([f(z^+)>f(z_{i+1})]+[f(z^+)>f(z_{i})])) \notag \\
			& = \frac{1}{2m^-m^+}\sum_{i=1}^{m-1}\sum_{z^+\in D^+}\sum_{z^-\in D^-}[f(z)=f(z_{i+1})]([f(z^+)>f(z_{i+1})]+[f(z^+)>f(z_{i})]) \notag \\
			& = \frac{1}{2m^-m^+}\sum_{z^+\in D^+}(\sum_{i=1}^{m-1}\sum_{z^-\in D^-}[f(z)=f(z_{i+1})])([f(z^+)>f(z_{i+1})]+[f(z^+)>f(z_{i})]) \notag \\
			& = \frac{1}{2m^-m^+}\sum_{z^+\in D^+}\sum_{z^-\in D^-}([f(z^+)>f(n(z^-))]+[f(z^+)>f(z^-)])\notag
		\end{align}

		Then 
		\[1-AUC = \frac{1}{2m^-m^+}\sum_{z^+\in D^+}\sum_{z^-\in D^-}([f(z^+)\leq f(n(z^-))]+[f(z^+)\leq f(z^-)])\]

		If $f(z^+)==f(z^-)$, then the $[f(z^+)\leq f(z^-)]$ holds but $[f(z^+)\leq f(n(z^-))]$ doesn't. If $f(z^+)<f(z^-)$, they both holds. Thus,
		\[1-AUC = \frac{1}{2m^-m^+}\sum_{z^+\in D^+}\sum_{z^-\in D^-}(2[f(z^+)< f(n(z^-))]+[f(z^+)\leq f(z^-)])=l_{rank}\]
	\end{solution}
	
	\section{[Bonus 10pts]Expected Prediction Error}
	For least squares linear regression problem, we assume our linear model as:\\
	\begin{equation}
	y=x^T \beta+\epsilon,
	\end{equation}
	where $\epsilon$ is noise and follows $\epsilon\sim N(0,\sigma^2)$. Note the instance feature of training data $\mathcal{D}$ as $\bm{X}\in\mathbb{R}^{p\times m}$ and note the label as $\bm{Y}\in\mathbb{R}^n$, where $n$ is the number of instance and $p$ is the feature dimension. So the estimation of model parameter is:\\
	\begin{equation}
	\hat{\beta}=(\bm{XX}^T)^{-1}\bm{XY}.
	\end{equation}
	For some given test instance $x_0$, please proof the expected prediction error $\text{\bf{EPE}}(x_0)$ follows:\\
	\begin{equation}
	\text{\bf{EPE}}(x_0)=\sigma^2+\mathbb{E}_{\mathcal{D}}[x^T_0(\bm{XX}^T)^{-1} x_0\sigma^2].
	\end{equation}
	Please give the steps and details of your proof.(Hint: $\text{\bf{EPE}}(x_0)=\mathbb{E}_{y_0|x_0}\mathbb{E}_{\mathcal{D}}[(y_0-\hat{y}_0)^2]$, you can also refer to the proof progress of variance-bias decomposition on the page 45 of our reference book)

	\begin{solution}
		\begin{align}
		\text{EPE}(x_0) & =E_{y_0|x_0}E_D((y_0-\hat y_0)^2)\notag\\
						& =E_{y_0|x_0}E_D((y_0-x_0^T\beta) + (x_0^T\beta-\hat y_0))^2\notag\\
						& =E_{y_0|x_0}E_D((y_0-x_0^T\beta)^2) + E_{y_0|x_0}E_D((x_0^T\beta-\hat y_0)^2)\notag\\
						& = \sigma^2 + E_D(E_{y_0|x_0}(x_0^T\beta-\hat y_0)^2))\notag\\
						& = \sigma^2 + E_D(x_0^T(XX^T)^{-1}x_0\sigma^2)\notag
		\end{align}
	\end{solution}
	